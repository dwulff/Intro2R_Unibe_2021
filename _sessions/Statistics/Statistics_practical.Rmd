---
title: "Statistics"
author: "<table style='table-layout:fixed;width:100%;border:0;padding:0;margin:0'><col width='10%'><col width='10%'>
  <tr style='border:none'>
    <td style='display:block;width:100%;text-align:left;vertical-align:bottom;padding:0;margin:0;border:none' nowrap>
      <font style='font-style:normal'>Introduction to R</font><br>
      <a href='https://therbootcamp.github.io/SmR_2021Apr/'>
        <i class='fas fa-clock' style='font-size:.9em;' ></i>
      </a>
      <a href='https://therbootcamp.github.io'>
        <i class='fas fa-home' style='font-size:.9em;'></i>
      </a>
      <a href='mailto:therbootcamp@gmail.com'>
        <i class='fas fa-envelope' style='font-size: .9em;'></i>
      </a>
      <a href='https://www.linkedin.com/company/basel-r-bootcamp/'>
        <i class='fab fa-linkedin' style='font-size: .9em;'></i>
      </a>
      <a href='https://therbootcamp.github.io'>
        <font style='font-style:normal'>The R Bootcamp</font>
      </a>
    </td>
    <td style='width:100%;vertical-align:bottom;text-align:right;padding:0;margin:0;border:none'>
      <img src='https://raw.githubusercontent.com/therbootcamp/therbootcamp.github.io/master/_sessions/_image/by-sa.png' style='height:15px;width:80px'/>
    </td>
  </tr></table>"
output:
  html_document:
    css: practical.css
    self_contained: no
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(comment = NA, 
                      fig.width = 6, 
                      fig.height = 6,
                      fig.align = 'center',
                      echo = TRUE, 
                      eval = TRUE, 
                      warning = FALSE)

options(digits = 3)

# Load packages
library(tidyverse)

# wine <- 
# wein %>%
#   rename("Quality" = "Qualität",
#          "Color" = "Farbe",
#          "Dissolved_Acid" = "Gelöste_Säure",
#          "Free_Acid" = "Freie_Säure",
#          "Citric_Acid" = "Citronensäure",
#          "Residual_Sugar" = "Restzucker",
#          "Chloride" = "Chloride",
#          "Free_Sulfur_Dioxide" = "Freie_Schwefeldioxide",
#          "Total_Sulfur_Dioxide" = "Gesamt_Schwefeldioxide",
#          "Density" = "Dichte",
#          "PH_Value" = "pH_Wert",
#          "Sulphate" = "Sulphate",
#          "Alcohol" = "Alkohol") %>%
#   mutate(Color = case_when(Color == 'weiss' ~ 'white',
#                              Color == 'rot' ~ 'red'))
# 
# write_csv(wine, '1_Data/wine.csv')

# generate english version

# Load packages
wine <- read_csv("1_Data/wine.csv") 
```

<p align="center" width="100%">
  <img src="image/vinho2.png" alt="Trulli" style="width:100%">
  <br>
  <font style="font-size:10px">from <a href="https://www.delinat.com/winelese-blog/wie-lange-ist-ein-wine-haltbar/">delinat.com</a></font>
</p>


# {.tabset}

## Overview

At the end of this practical you will be able to ...

1. use categorical variables as predictors in a regression 
2. know how to add an interaction effect and understand why it might be important to standardize your variables

## Tasks

### A - Setup

1. Open your `BernRBootcamp` R project. It should already have the folders `1_Data` and `2_Code`. Make sure that the data files listed in the `Datasets` section above are in your `1_Data` folder.

2. Using `library()` load the set of packages for this practical listed in the Functions section above.

```{r, eval = FALSE, echo = TRUE}
## Name
## Date
## Statistics Practical

library(XX)     
library(XX)
```

3. For this practical we will use a dataset called `wine.csv` which you will now import with `read_csv()`.

```{r, echo=T, eval = T, message = F}
# Read in the data
wine <- read_csv(file = "1_Data/wine.csv")
```

4. Execute the code below to ensure that all `character` variables are converte to factors. This will help the statistical model to interpret categorical variables correctly. 

```{r, echo = TRUE}
# convert character to factor
wine <- wine %>% mutate_if(is.character, factor)
```


### B - Comparing groups: <i>t</i>-test versus regression

1. In this part we will inspect the effect of `Color` of the wine, red or white, as a predictor for `Quality`. Use the code below to generate two vectors that include quality ratings for white and red wine. 

```{r, echo = TRUE}
# Qualitiy vectors by color
white <- wine %>% filter(Color == 'white') %>% pull(Quality)
red <- wine %>% filter(Color == 'red') %>% pull(Quality)
```

2. Use the `t.test()` template below to compare the two vectors with a <i>t</i>-test. You do not have to save the result.

```{r, eval = FALSE, echo = TRUE}
# t-test
t.test(x = XX, y = XX)
```

```{r}
# t-test
t.test(x = white, y = red)
```

3. What does the output of the <i>t</i>-tests tell you about the difference between white and red wine in perceived quality? You will find the answer in the second line of the output that starts with `t=..` and in the last line.  

4. White wine got a higher rating with `0.2419` (difference of the two means) points more than red wine, this difference is significant. Now, try to get the same result with a regression. Predict `Quality` with `Color`.

```{r, eval = FALSE, echo = TRUE}
# Regression
wine_lm <- lm(formula = XX ~ XX, 
              data = XX)
```

```{r}
# Regression
wine_lm <- lm(formula = Quality ~ Color, 
              data = wine)
```

5. Print the object and inspect the regression weights. Do you recognize some of these numbers?

6. Exactly! The regression weight for `Color` is the mean difference of red and white wines. What does the intercept represent? It represents the value of the category that got assigned 0 by R. As this variable is `character` the default is assigned to the category that comes earlier in the alphabet, i.e., `'red' < 'white'`.   

7. Now use the `summary()` function on your model object and compare the degrees of freedom, <i>t</i>- and p-values with the ones from the <i>t</i>-test above. 

```{r}
# summary
summary(wine_lm)
```

8. The values from the t-test do not exactly match the estimations for the weights in the regression. This stems from the fact that the `t.test()` function allows the variance of the groups (red and white) to be different. In contrast regression always assumes that the variances between groups are the same. Re-calculate the <i>t</i>-test with the argument `var.equal = TRUE`.   

```{r, eval = FALSE, echo = TRUE}
# t-test
t.test(x = XX, y = XX, var.equal = XX)
```

```{r}
# t-test
t.test(x = white, y = red, var.equal = TRUE)
```

9. Order has been resumed! All values in the <i>t</i>-test and the regression should be identical now. 

### C - Comparing groups: Coding

1. The default in R for dummy coding is 0 for one feature and 1 for the other. Alternatively you can code effects, with the values -1 and 1. To see the difference and consequences of these different codings generate two new varibales in your dataset using the code below. 

```{r, echo = TRUE}
# Kodierungen der Color
wine <- wine %>% mutate(Color_dummy = ifelse(Color == 'red', 0, 1),
                        Color_effect = ifelse(Color == 'red', -1, 1))
```

2. Calculate two regressions now, one with dummy coding and the other with effect coding as the predictor and save them as new objects called `wine_dummy` and `wine_effect`.   

```{r, eval = FALSE, echo = TRUE}
# Regression dummy
wine_dummy <- lm(formula = XX ~ XX, 
                 data = XX)

# Regression effekt
wine_effect <- lm(formula = XX ~ XX, 
                  data = XX)
```

```{r}
# Regression dummy
wine_dummy <- lm(formula = Quality ~ Color_dummy, 
                 data = wine)

# Regression effekt
wine_effect <- lm(formula = Quality ~ Color_effect, 
                  data = wine)
```

3. Print these objects now and compare the weights. The dummy coding weights should be familiar. How do they compare to the effect coding? Do you see the connection?

4. The weights for color using effect coding is exactly half of the weight for dummy coding, to compensate for this change the intercept changed for exactly the same value. Verify the weights with the calculations done in the code snippet below.

```{r, echo = TRUE}
# Dummy-Kodierung
mean(red) # intercept
mean(white) - mean(red) # gewicht Color

# EffekKodierung
(mean(red) + mean(white))/2 # intercept
mean(white) - (mean(red) + mean(white))/2 # gewicht Color
```

5. Compare the models now with `summary()`. Where can you find differences, what stays the same?

```{r}
# Regression dummy
summary(wine_dummy)

# Regression effekt
summary(wine_effect)
```

6. The coding (dummy or effect) has, with the exception of the weights scaling and the standard error, no influence on the <i>t</i>-value and the <i>p</i>-value. Only the intercept changes dramatically, because in the effect coding the intercept is further away from zero.

### D - Interactions

1. In the sections above we learned that white wine is preferred over red wine. Could it be the case that controlling for other variables might change this? Or could it be the case that this difference only excists under special circumstances, i.e., that there are moderators for the effect of color? Calculate a regression, that adds `Alcohol` as a predictor using `*`. This will also calculate the interactions between the predictors.   

```{r, eval = FALSE, echo = TRUE}
# Regression mit Interaktion
wine_lm <- lm(formula = XX ~ XX * XX, 
              data = XX)
```

```{r}
# Regression mit Interaktion
wine_lm <- lm(formula = Quality ~ Color * Alcohol, 
              data = wine)
```

2. Print the object and inspect the weights. How do you interpret these values?

3. You can interprete the weights in the following manner: Considering `Alcohol` and the interaction white wines are rated  `.7` points better than red wines. Considering `Color` and the interaction results an increase of one volume percent results in an increase of perceived quality of `.36`. Considering `Color` and `Alcohol` the interaction, i.e., the product of the two predictors, leads to a change of `-.05`. This means that the effect of `Alcohol` considering  white wine is reduced for exactly this value. In other words, the effect of `Color` for wine with a lot of alcohol is smaller. Use the code below to visualize your results. Yellow means white wine, black red wine.

```{r, echo =T}
# Visualisierung
ggplot(data = wine, 
       aes(x = Alcohol, y = Quality, col = Color, alpha = .01)) + 
  scale_color_manual(values = viridis::cividis(2)) + 
  geom_jitter(width=2,height=1.5,size=.1) + theme_minimal() + theme(legend.position = 'none') +
  geom_smooth(data = wine %>% filter(Color == 'white'), method = 'lm') + 
  geom_smooth(data = wine %>% filter(Color == 'red'), method = 'lm')

```

4. Using `summary()` check the <i>t</i>-values and p-values. Which predictors are significant?

5. Yes - all three predictors are significant! <i>t</i>- and p-values are the most extreme for `Alcohol`, but the weight for `Alcohol` is halfed. How can you explain this?  

6. Exactly! The weights are dependent on the scaling of the predictors! Re-calculate the regression, but with scaled predictors using the code below.

```{r, eval = FALSE, echo = TRUE}
# Skalierungsfunktion
scale_it = function(x) (x - mean(x))/sd(x)

# Regression mit skalierten Prädiktoren
wine_lm <- lm(formula = XX ~ XX * XX, 
              data = XX %>% mutate_if(is.numeric, scale_it))
```

```{r}
# Skalierungsfunktion
scale_it = function(x) (x - mean(x))/sd(x)

# Regression mit skalierten Prädiktoren
wine_lm <- lm(formula = Quality ~ Color * Alcohol, 
              data = wine %>% mutate_if(is.numeric, scale_it))
```

7. Print the object and inspect the weights. All values changed quite substantially. Especially `Alcohol` doubled its weight. How do you interpret these new weights, keeping in mind, that the scaling results in all variables having a standard deviation of 1? 

8. After scaling the weight can be interpreted as the changes in the standard deviation. For examples, a change of 1 standard deviation in `Alcohol` results in a change of `.4928` standard deviations in `Quality`. The intercept shows the biggest changes, how do you interpret this change?

9. The intercept is the estimated mean of the criterion, given that all predictors are zero. Without scaling it is tricky to interpret, cause zero does not necessarily have to be included in any of the predictors. After scaling this changes and all predictors, with the exception of `Color` have a mean of zero. This means the intercept represents a mean level of `Alcohol`, red wine and no interaction through the standard deviation. You can see this in the visualization. Use the code below and find the point on the line where `Alcohol` equals zero.

```{r, echo = T}
# Visualisierung
ggplot(data = wine %>% mutate_if(is.numeric, scale_it), 
       aes(x = Alcohol, y = Quality, col = Color, alpha = .01)) + 
  scale_color_manual(values = viridis::cividis(2))  +
  geom_jitter(width=2,height=1.5,size=.1) + theme_minimal() + theme(legend.position = 'none') +
  geom_smooth(data = wine %>% mutate_if(is.numeric, scale_it) %>% filter(Color == 'white'), method = 'lm') + 
  geom_smooth(data = wine %>% mutate_if(is.numeric, scale_it) %>% filter(Color == 'red'), method = 'lm')

```

10. Using `summary()` identify what changed regarding <i>t</i>-, p-values, standard error and R-squared? 

11. The main changes can be found for the variable `Color`, which was not scaled! Before scaling of `Alcohol`, `Color` was nearly perfectly correlated with the interaction, leading to an inflated standard error. Scaling reduced this correlation and the standard error is now of the level of the other predictors. In most models it is worth scaling the predictors (sometimes also the criterion), given that you are interested in the weights and significance levels. 

## Examples

```{r, eval = FALSE, echo = TRUE}
# Regression mit R

library(tidyverse)

# Model:
# Sagt der Hubraum (displ) die pro gallone 
# fahrbaren Meilen voraus?
hwy_mod <- lm(formula = hwy ~ displ,
               data = mpg)

# Ergebnisse 
summary(hwy_mod)
coef(hwy_mod)

# Gefittete Werte
hwy_fit <- fitted(hwy_mod)
hwy_fit

# Residuums 
hwy_resid <- residuals(hwy_mod)
hwy_resid

```


## Datasets

|Datei | Zeile | Spalte |
|:----|:-----|:------|
|[wine.csv](https://github.com/dwulff/Intro2R_Unibe_2021/raw/main/_sessions/Statistics/1_Data/wine.csv) | 6497 | 13 |



#### wine.csv

The `wine.csv` file contains data of the Comissão De Viticultura Da Região Dos Vinhos Verdes, the official certification agency of Vinho Verde in Portugal for the years 2004 to 2007.


| Name | Description |
|:-------------|:-------------------------------------|
|Quality | Quality rating on a scale between 1-9 |
|Color| red or white wine |
|Dissolved_Acid| Concentration of acids dissolved in the wine |
|Free_Acid| Concentration of free acids |
|Citric_Acid| Citric acid in the wine |
|Residual_Sugar| Sugar concentration in the wine|
|Chloride| Chlorid concentration in the|
|Free_Sulfur_Dioxide| Free sulfur dioxide in the |
|Total_Sulfur_Dioxide| Total amount of sulfur dioxide |
|Density|Density of the wine|
|PH_Value|pH-Value of the wine. The smaller the more acidic.|
|Sulphate| Sulphate concentration of the wine |
|Alcohol| Alcohol in the wine in %|

## Functions

### Packages

|Package| Installation|
|:------|:------|
|`tidyverse`|`install.packages("tidyverse")`|

### Functions

| Function| Package | Description |
|:---|:------|:---------------------------------------------|
|   `lm`|`stats`| Fit a linear model  |
|   `fitted`|`stats`| Extract predicted values|
|   `residuals`|`stats`| Extract residuals |

## Resources

### Books

- [Discovering Statistics with R](https://www.amazon.com/Discovering-Statistics-Using-Andy-Field/dp/1446200469) excellent and entertaining overview on statistics